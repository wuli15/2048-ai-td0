# 2048-TD(0)
___
## 概述
- 该项目的主体是基于强化学习完成的2048-ai。项目采用了TD(0)方法并通过离线学习加上反向更新提高收敛速度。同时，项目中带有DQN方法的实现，该部分仍在加工中，并未集成到主程序中。
- 主程序是2048游戏，为以前的作业内容，代码实现和组织存在一定的不足，采取内置tkinter控件进行gui绘制，实现了基础效果。该部分可替换成pygame等其他实现方式
- 程序中包括了基础  ***贪心算法***（最大合成一般在512以内）以及 ***蒙特卡洛***（约80%的2048成功率）
  - 贪心算法完全基于得分设计，该部分可以进行调整
  - 蒙特卡洛算法由于硬件限制控制了搜索的深度，同时奖励函数设置较为简单，该部分也可以进行修改
- 主程序中隐藏了调用大模型完成游戏的方法。由于大模型分析耗时长，且消耗token较多，故隐藏（如需使用需要自行在`ai_llm`模块中添加`api_key`等参数）
- 三种算法目前均不支持暂停操作
  
训练程序为`RL/train_td0.py`模型采用np数组存储，由于采用n-tuple查表方式进行参数存储，内存占用较大，因此不在项目中包含
___
## 训练说明（RL目录下）
1. 直接运行
    ```bash 
    python train_game.py
    ```
2. **~~设置参数运行~~**(ε,α_decay,γ,λ参数仍在适配测试中，暂时不支持自定义)
    ```bash
    python train_game.py -h   #查看具体参数
    ```
   *~~示例~~*：
    ```bash
   python train_td0.py --ep 100000 --lr 0.1 --epsilon 0.1 --lr_deacy 1 --g 0.97 --lam 0.95 --s 15
    ```
- 目前暂不支持自定义保存路径，模型存储在models/model.npy，训练轮数直接存储在同级文件夹下
___
## 训练效果说明
- 每10轮输出当前轮数最终局面以及分数，如：
  ```
    ╭─────┬─────┬─────┬─────╮
    │  512┊ 1024┊ 4096┊ 8192│
    ├┄┄┄┄┄┼┄┄┄┄┄┼┄┄┄┄┄┼┄┄┄┄┄┤
    │   32┊   64┊  128┊  256│
    ├┄┄┄┄┄┼┄┄┄┄┄┼┄┄┄┄┄┼┄┄┄┄┄┤
    │    4┊    8┊   32┊    4│
    ├┄┄┄┄┄┼┄┄┄┄┄┼┄┄┄┄┄┼┄┄┄┄┄┤
    │    2┊    4┊   16┊    2│
    ╰─────┴─────┴─────┴─────╯
    episode:89640, score:157192
  ```
- 每100轮输出最近100轮的方块合成率

## 训练效果
- 使用默认参数训练150000轮的最终测试效果：
  ```
    Total episodes: 1000
    Total time: 94.74 seconds
    512: 99.60%
    1024: 98.20%
    2048: 91.10%
    4096: 71.10%
    8192: 18.90%
  ```
  
### 补充说明
- 本代码完全独立编写，但在学习率调整和n-tuple的设计上参考了github moporgic/TDL2048-Demo的部分设计思路